#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import argparse
import json
import logging
from datetime import datetime, timedelta
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.models.tweet import Tweet
from src.models.project import Project

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def format_impact_level(impact_level):
    """æ ¼å¼åŒ–å½±å“ç­‰çº§ï¼Œæ·»åŠ è¡¨æƒ…ç¬¦å·"""
    if impact_level == "Extremely Bullish":
        return "ğŸŸ¢ğŸŸ¢ æåº¦çœ‹æ¶¨"
    elif impact_level == "Bullish":
        return "ğŸŸ¢ çœ‹æ¶¨"
    elif impact_level == "Non-Significant":
        return "âšª æ— æ˜¾è‘—å½±å“"
    elif impact_level == "Bearish":
        return "ğŸ”´ çœ‹è·Œ"
    elif impact_level == "Extremely Bearish":
        return "ğŸ”´ğŸ”´ æåº¦çœ‹è·Œ"
    else:
        return impact_level

def print_tweet_info(tweet):
    """æ‰“å°æ¨æ–‡ä¿¡æ¯"""
    # æŸ¥æ‰¾é¡¹ç›®ä¿¡æ¯
    project = Project.get_by_id(tweet.project_id)
    project_name = project.name if project else "æœªçŸ¥é¡¹ç›®"
    
    # æ ¼å¼åŒ–æ—¶é—´
    created_at = tweet.created_at
    if isinstance(created_at, str):
        try:
            created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
        except:
            pass
    
    if isinstance(created_at, datetime):
        created_at_str = created_at.strftime("%Y-%m-%d %H:%M:%S")
    else:
        created_at_str = str(created_at)
    
    # åˆ†æç»“æœ
    analysis = tweet.analysis or {}
    impact_level = analysis.get('impact_level', 'æœªåˆ†æ')
    expected_volatility = analysis.get('expected_volatility', 'æœªçŸ¥')
    event_type = analysis.get('event_type', 'æœªåˆ†ç±»')
    key_factors = analysis.get('key_factors', [])
    historical_reference = analysis.get('historical_reference', 'æ— å†å²å‚ç…§')
    
    # æ‰“å°ä¿¡æ¯
    logger.info("=" * 70)
    logger.info(f"æ¨æ–‡ID: {tweet.tweet_id}")
    logger.info(f"é¡¹ç›®: {project_name} ({tweet.token_symbol})")
    logger.info(f"Twitterè´¦å·: @{tweet.twitter_username}")
    logger.info(f"å‘å¸ƒæ—¶é—´: {created_at_str}")
    logger.info(f"å½±å“ç­‰çº§: {format_impact_level(impact_level)}")
    logger.info(f"é¢„æœŸæ³¢åŠ¨: {expected_volatility}")
    logger.info(f"äº‹ä»¶ç±»å‹: {event_type}")
    logger.info(f"æ¨æ–‡å†…å®¹:\n{tweet.text}")
    
    if key_factors:
        logger.info("\nå…³é”®å› ç´ :")
        for i, factor in enumerate(key_factors, 1):
            logger.info(f"{i}. {factor}")
    
    logger.info(f"\nå†å²å‚ç…§: {historical_reference}")
    logger.info("-" * 70)

def query_recent_tweets(args):
    """æŸ¥è¯¢æœ€è¿‘çš„æ¨æ–‡"""
    try:
        limit = args.limit
        tweets = Tweet.get_recent_tweets(limit=limit)
        
        if not tweets:
            logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨æ–‡")
            return True
        
        logger.info(f"æœ€è¿‘ {len(tweets)} æ¡æ¨æ–‡:")
        
        for tweet in tweets:
            print_tweet_info(tweet)
        
        return True
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æœ€è¿‘æ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
        return False

def query_project_tweets(args):
    """æŸ¥è¯¢ç‰¹å®šé¡¹ç›®çš„æ¨æ–‡"""
    try:
        project_id = args.project_id
        limit = args.limit
        
        if not project_id:
            # æŸ¥è¯¢é¡¹ç›®åˆ—è¡¨
            projects = Project.get_all()
            if not projects:
                logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®")
                return True
            
            logger.info("è¯·é€‰æ‹©ä¸€ä¸ªé¡¹ç›®:")
            for i, project in enumerate(projects, 1):
                logger.info(f"{i}. {project.name} ({project.token_symbol})")
            
            choice = input("è¯·è¾“å…¥é¡¹ç›®ç¼–å·: ")
            try:
                index = int(choice) - 1
                if 0 <= index < len(projects):
                    project_id = str(projects[index]._id)
                else:
                    logger.error("æ— æ•ˆçš„é¡¹ç›®ç¼–å·")
                    return False
            except ValueError:
                logger.error("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                return False
        
        # æŸ¥æ‰¾é¡¹ç›®ä¿¡æ¯
        project = Project.get_by_id(project_id)
        if not project:
            logger.error(f"æ‰¾ä¸åˆ°IDä¸º {project_id} çš„é¡¹ç›®")
            return False
        
        logger.info(f"æŸ¥è¯¢é¡¹ç›®: {project.name} ({project.token_symbol})")
        
        # æŸ¥è¯¢é¡¹ç›®æ¨æ–‡
        tweets = Tweet.get_project_tweets(project_id, limit=limit)
        
        if not tweets:
            logger.info("è¯¥é¡¹ç›®æ²¡æœ‰ä»»ä½•æ¨æ–‡è®°å½•")
            return True
        
        logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡:")
        
        for tweet in tweets:
            print_tweet_info(tweet)
        
        return True
    except Exception as e:
        logger.error(f"æŸ¥è¯¢é¡¹ç›®æ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
        return False

def query_impact_tweets(args):
    """æŸ¥è¯¢ç‰¹å®šå½±å“ç­‰çº§çš„æ¨æ–‡"""
    try:
        impact_level = args.impact_level
        limit = args.limit
        
        if not impact_level:
            # æ˜¾ç¤ºå¯é€‰çš„å½±å“ç­‰çº§
            impact_levels = [
                "Extremely Bullish",
                "Bullish",
                "Non-Significant",
                "Bearish",
                "Extremely Bearish"
            ]
            
            logger.info("è¯·é€‰æ‹©å½±å“ç­‰çº§:")
            for i, level in enumerate(impact_levels, 1):
                logger.info(f"{i}. {format_impact_level(level)}")
            
            choice = input("è¯·è¾“å…¥å½±å“ç­‰çº§ç¼–å·: ")
            try:
                index = int(choice) - 1
                if 0 <= index < len(impact_levels):
                    impact_level = impact_levels[index]
                else:
                    logger.error("æ— æ•ˆçš„å½±å“ç­‰çº§ç¼–å·")
                    return False
            except ValueError:
                logger.error("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                return False
        
        # æŸ¥è¯¢æ¨æ–‡
        tweets = Tweet.get_by_impact_level(impact_level, limit=limit)
        
        if not tweets:
            logger.info(f"æ²¡æœ‰æ‰¾åˆ°å½±å“ç­‰çº§ä¸º {format_impact_level(impact_level)} çš„æ¨æ–‡")
            return True
        
        logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡å½±å“ç­‰çº§ä¸º {format_impact_level(impact_level)} çš„æ¨æ–‡:")
        
        for tweet in tweets:
            print_tweet_info(tweet)
        
        return True
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å½±å“ç­‰çº§æ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
        return False

def export_tweets(args):
    """å¯¼å‡ºæ¨æ–‡æ•°æ®ä¸ºJSONæ–‡ä»¶"""
    try:
        output_file = args.output
        project_id = args.project_id
        limit = args.limit
        
        # æŸ¥è¯¢æ¨æ–‡
        if project_id:
            tweets = Tweet.get_project_tweets(project_id, limit=limit)
            file_prefix = f"project_{project_id}"
        else:
            tweets = Tweet.get_recent_tweets(limit=limit)
            file_prefix = "all_tweets"
        
        if not tweets:
            logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨æ–‡")
            return True
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"{file_prefix}_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        tweets_data = [tweet.to_dict() for tweet in tweets]
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(tweets_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·²å°† {len(tweets)} æ¡æ¨æ–‡å¯¼å‡ºåˆ° {output_file}")
        return True
    except Exception as e:
        logger.error(f"å¯¼å‡ºæ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
        return False

def main():
    # åˆ›å»ºä¸»è§£æå™¨
    parser = argparse.ArgumentParser(description='XMonitoræ¨æ–‡æŸ¥è¯¢å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æŸ¥è¯¢æœ€è¿‘æ¨æ–‡å­å‘½ä»¤
    recent_parser = subparsers.add_parser('recent', help='æŸ¥è¯¢æœ€è¿‘çš„æ¨æ–‡')
    recent_parser.add_argument('--limit', '-l', type=int, default=10, help='æœ€å¤§è¿”å›æ•°é‡')
    recent_parser.set_defaults(func=query_recent_tweets)
    
    # æŸ¥è¯¢é¡¹ç›®æ¨æ–‡å­å‘½ä»¤
    project_parser = subparsers.add_parser('project', help='æŸ¥è¯¢ç‰¹å®šé¡¹ç›®çš„æ¨æ–‡')
    project_parser.add_argument('--project-id', '-p', help='é¡¹ç›®ID')
    project_parser.add_argument('--limit', '-l', type=int, default=10, help='æœ€å¤§è¿”å›æ•°é‡')
    project_parser.set_defaults(func=query_project_tweets)
    
    # æŸ¥è¯¢å½±å“ç­‰çº§æ¨æ–‡å­å‘½ä»¤
    impact_parser = subparsers.add_parser('impact', help='æŸ¥è¯¢ç‰¹å®šå½±å“ç­‰çº§çš„æ¨æ–‡')
    impact_parser.add_argument('--impact-level', '-i', help='å½±å“ç­‰çº§')
    impact_parser.add_argument('--limit', '-l', type=int, default=10, help='æœ€å¤§è¿”å›æ•°é‡')
    impact_parser.set_defaults(func=query_impact_tweets)
    
    # å¯¼å‡ºæ¨æ–‡å­å‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡ºæ¨æ–‡æ•°æ®ä¸ºJSONæ–‡ä»¶')
    export_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶å')
    export_parser.add_argument('--project-id', '-p', help='é¡¹ç›®IDï¼Œå¦‚ä¸æŒ‡å®šåˆ™å¯¼å‡ºæ‰€æœ‰æ¨æ–‡')
    export_parser.add_argument('--limit', '-l', type=int, default=100, help='æœ€å¤§å¯¼å‡ºæ•°é‡')
    export_parser.set_defaults(func=export_tweets)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # æ‰§è¡Œç›¸åº”çš„åŠŸèƒ½
    args.func(args)

if __name__ == '__main__':
    main() 